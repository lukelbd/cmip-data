#!/usr/bin/env python3
"""
Download and filter datasets using the ESGF python API.

Important
---------
The scripts generated by these functions have two approaches for validating users:
either on-the-fly with the '-H' flag (requiring users to the password in a prompt)
or using some temporary certificate stored in '~/.esg/credentials.pem' (automatically
generated whenever `pyesgf.LogonManager` is instantiated). It seems only the latter
approach invokes 'wget' while former uses a less informative downloader. To use the
latter might have to delete the certificate file and instantiate a new logon manager.
"""
import builtins
import copy
import re
from pathlib import Path

from pyesgf.logon import LogonManager
from pyesgf.search import SearchConnection

from .internals import (
    FACETS_FOLDER,
    FACETS_SUMMARY,
    FLAGSHIP_ENSEMBLES,
    Database,
    Printer,
    _glob_files,
    _item_file,
    _item_join,
    _item_years,
    _node_parts,
    _parse_constraints,
    _sort_facets,
)

__all__ = [
    'init_connection',
    'download_script',
    'filter_script',
    'summarize_downloads',
]

# Global constants
# ESGF hosts for logon and nodes for downloading
# Hosts are listed here: https://esgf.llnl.gov/nodes.html
# Nodes and statuses are listed here: https://esgf-node.llnl.gov/status/
# LLNL: 11900116 hits for CMIP6 (best!)
# DKRZ: 01009809 hits for CMIP6
# IPSL: 01452125 hits for CMIP6
NODES_HOSTS = {
    'llnl': 'esgf-node.llnl.gov',
    'ceda': 'esgf-index1.ceda.ac.uk',
    'dkrz': 'esgf-data.dkrz.de',
    'gfdl': 'esgdata.gfdl.noaa.gov',
    'ipsl': 'esgf-node.ipsl.upmc.fr',
    'jpl': 'esgf-node.jpl.nasa.gov',
    'liu': 'esg-dn1.nsc.liu.se',
    'nci': 'esgf.nci.org.au',
    'nccs': 'esgf.nccs.nasa.gov',
}
NODES_DATASETS = [
    'aims3.llnl.gov',
    'cmip.bcc.cma.cn',
    'cmip.dess.tsinghua.edu.cn',
    'cmip.fio.org.cn',
    'cordexesg.dmi.dk',
    'crd-esgf-drc.ec.gc.ca',
    'data.meteo.unican.es',
    'dataserver.nccs.nasa.gov',
    'dist.nmlab.snu.ac.kr',
    'dpesgf03.nccs.nasa.gov',
    'eridanus.eoc.dlr.de',
    'esg-cccr.tropmet.res.in',
    'esg-dn1.nsc.liu.se',
    'esg-dn1.ru.ac.th',
    'esg-dn2.nsc.liu.se',
    'esg.camscma.cn',
    'esg.lasg.ac.cn',
    'esg.pik-potsdam.de',
    'esg1.umr-cnrm.fr',
    'esg2.umr-cnrm.fr',
    'esgdata.gfdl.noaa.gov',
    'esgf-cnr.hpc.cineca.it',
    'esgf-data.csc.fi',
    'esgf-data.ucar.edu',
    'esgf-data1.ceda.ac.uk',
    'esgf-data1.diasjp.net',
    'esgf-data1.llnl.gov',
    'esgf-data2.ceda.ac.uk',
    'esgf-data2.diasjp.net',
    'esgf-data2.llnl.gov',
    'esgf-data3.ceda.ac.uk',
    'esgf-data3.diasjp.net',
    'esgf-ictp.hpc.cineca.it',
    'esgf-nimscmip6.apcc21.org',
    'esgf-node.cmcc.it',
    'esgf-node2.cmcc.it',
    'esgf.anl.gov',
    'esgf.apcc21.org',
    'esgf.bsc.es',
    'esgf.dwd.de',
    'esgf.ichec.ie',
    'esgf.nccs.nasa.gov',
    'esgf.nci.org.au',
    'esgf.rcec.sinica.edu.tw',
    'esgf1.dkrz.de',
    'esgf2.dkrz.de',
    'esgf3.dkrz.de',
    'noresg.nird.sigma2.no',
    'polaris.pknu.ac.kr',
    'vesg.ipsl.upmc.fr'
]


def _parse_script(arg, complement=False):
    """
    Return the download lines of the wget scripts or their complement. The latter is
    used when constructing a single script from many scripts.

    Parameters
    ----------
    arg : str or path-like
        The string content or `pathlib.Path` location.
    complement : bool, default: False
        Whether to return the filename lines or the preceding and succeeding lines.
    """
    if isinstance(arg, Path):
        lines = open(arg, 'r').readlines()
    else:
        lines = [_ + '\n' for _ in arg.split('\n')]
    eof = 'EOF--dataset.file.url.chksum_type.chksum'  # marker for download lines
    idxs = [i for i, line in enumerate(lines) if eof in line]
    if idxs and len(idxs) != 2:
        raise NotImplementedError
    if complement:
        if not idxs:
            raise NotImplementedError
        return lines[:idxs[0] + 1], lines[idxs[1]:]  # return tuple of pairs
    else:
        if idxs:
            lines = lines[idxs[0] + 1:idxs[1]]
        return lines


def _write_script(
    path, prefix, center, suffix, openid=None, printer=None, **constraints
):
    """
    Save the wget script to the specified path. Lines are sorted first by model,
    second by variable, and third by node priority.

    Parameters
    ----------
    path : path-like
        The script location.
    prefix, center, suffix : list
        The script line parts. The center lines are sorted.
    openid : str, optional
        The openid to be hardcoded in the file.
    printer : callable, optional
        The print function.
    **constraints
        The constraints.
    """
    # NOTE: This repairs the script to skip the modification time check and only
    # performs the checksum comparison when the checksum type (i.e. MDA5 or SHA256)
    # in the status file is the same as the type in the script (by comparing lengths).
    # NOTE: The download status logs list files only once because the script code
    # replaces existing file lines. Try: for f in ./*/.wget*; do echo "File: $f"
    # cat $f | cut -d' '| wc -l && cat $f | cut -d' ' -f1 | sort | uniq | wc -l; done
    # WARNING: Script often recommends using --update to update files but this is
    # usually because different nodes offer the same file with different hashes. So
    # better to monitor and manually remove invalid files -- never blindly re-download.
    openid = openid or ''
    print = printer or builtins.print
    path = Path(path).expanduser()
    path.mkdir(exist_ok=True)
    _, constraints = _parse_constraints(reverse=True, **constraints)
    name = _item_join(*constraints.values())  # e.g. ts-ta_amon_picontrol-abrupt4xco2
    path = path / ('wget_' + name + '.sh')
    prefix = re.sub(r'openId=\n', f'openId={openid!r}\n', ''.join(prefix))
    if openid and not re.search(re.escape(f'{openid!r}'), prefix):
        print('Warning: Failed to substitute in user openid.')
    replace = r'-r "$file"'  # only check existence (ignore changes after download)
    for code in (  # loop over default code and outdated replacement code
        '"$(get_mod_time_ $file)" == $(echo "$cached" | cut -d \' \' -f2)',
        r'-n "$(awk "$(get_mod_time_ $file) >= \$2 { print \$2 }" <<< "$cached")"',
    ):
        suffix = re.sub(re.escape(code), replace, ''.join(suffix))
    if not re.search(re.escape(replace), suffix):
        print('Warning: Failed to repair modification time comparison line.')
    replace = (  # skip comparisons of different checksum types
        '"$chksum" == "$(echo "$cached" | cut -d\' \' -f3)" || '
        '"$(echo "$chksum" | wc -c)" != "$(echo "$cached" | cut -d\' \' -f3 | wc -c)"'
    )  # WARNING: use cut -d without space to prevent replacing existing replacement
    for code in (  # loop over default code and outdated replacement code
        '"$chksum" == "$(echo "$cached" | cut -d \' \' -f3)"',
        r'-n "$(awk "\"$chksum\" == \$3 { print \$3 }" <<< "$cached")"',
    ):
        suffix = re.sub(re.escape(code), replace, suffix)
    if not re.search(re.escape(replace), suffix):
        print('Warning: Failed to repair file checksum comparison line.')
    center = _sort_facets(center, tuple(_node_parts))  # sort with default order
    with open(path, 'w') as f:
        f.write(prefix + ''.join(center) + suffix)
    path.chmod(0o755)
    ntotal, nunique = len(center), len(set(map(_item_file, center)))
    print(f'Output script ({ntotal} total files, {nunique} unique files): {path}\n')
    return path


def init_connection(node='llnl', username=None, password=None):
    """
    Initialize a distributed search connection over the specified node
    with the specified user information.

    Parameters
    ----------
    node : str, default: 'llnl'
        The ESGF node to use for searching.
    username : str, optional
        The username for logging on.
    password : str, optional
        The password for logging on.
    """
    # Log on and initalize the connection using requested node
    node = node.lower()
    urls = NODES_HOSTS
    if node in urls:
        host = urls[node]
    elif node in urls.values():
        host = node
    else:
        raise ValueError(f'Invalid node {node!r}.')
    lm = LogonManager()
    if not lm.is_logged_on():  # surface orography
        lm.logon(username=username, password=password, hostname=host)
    url = f'https://{host}/esg-search'  # suffix is critical
    return SearchConnection(url, distrib=True)


def download_script(
    path='~/data', node='llnl', username=None, password=None, openid=None,
    dataset_filter=None, flagship_filter=False, **constraints
):
    """
    Download a wget script using `pyesgf`. The resulting script can be filtered to
    particular years or intersecting constraints using `filter_script`.

    Parameters
    ----------
    path : path-like, default: '~/data'
        The output path for the resulting wget file.
    node, username, password : str, defaultflagship_filter 'llnl'
        Passed to `connect_node`.
    openid : str, optional
        The openid to hardcode into the resulting wget script.
    dataset_filter : callable, optional
        Function that returns whether to retain a dataset id.
    flagship_filter : bool, optional
        Whether to select only flagship CMIP5 and CMIP6 ensembles.
    **constraints
        Constraints passed to `~pyesgf.search.SearchContext`.
    """
    # Translate constraints and possibly apply flagship filter
    # NOTE: The datasets returned by .search() will often be 'empty' but this usually
    # means it is not available from a particular node and available at another node.
    # NOTE: Thousands of these scripts can take up significant space... so instead we
    # consolidate results into a single script. Also increase the batch size from the
    # default of only 50 results to reduce the number of http requests.
    project, constraints = _parse_constraints(restrict=False, **constraints)
    print = Printer('download', **constraints)
    conn = init_connection(node, username, password)
    if flagship_filter:
        ensembles = [ens for key, ens in FLAGSHIP_ENSEMBLES.items() if key[0] == project]  # noqa: E501
        if not ensembles:
            raise ValueError(f'Invalid {project=} for {flagship_filter=}. Must be CMIP5 or CMIP6.')  # noqa: E501
        _, constraints = _parse_constraints(restrict=False, ensemble=ensembles, **constraints)  # noqa: E501
        def flagship_filter(value):  # noqa: E301
            identifiers = [s.lower() for s in value.split('.')]
            for keys, ensemble in FLAGSHIP_ENSEMBLES.items():
                if all(key and key.lower() in identifiers for key in (*keys, ensemble)):
                    return True
            else:
                return FLAGSHIP_ENSEMBLES[project] in identifiers

    # Search and parse wget scripts
    # NOTE: This uses the 'dataset' search context to find individual simulations (i.e.
    # 'datasets' in ESGF parlance) then creates a 'file' search context within the
    # individual dataset and generates files form each list.
    # NOTE: Since cmip5 'datasets' often contain multiple variables, calling search()
    # on the DatasetSearchContext returned by new_context() then get_download_script()
    # on the resulting DatasetResult could include unwanted files. Therefore use
    # FileContext.constrain() to re-apply constraints to files within each dataset
    # (can also pass constraints to search() or get_download_script(), which both
    # call constrain() internally). This is in line with other documented approaches for
    # both the GUI and python API. See the below pages (the gitlab notebook also filters
    # out duplicate files but mentions the advantage of preserving all duplicates in
    # case one download node fails, which is what we use in our wget script workflow):
    # https://claut.gitlab.io/man_ccia/lab2.html#searching-and-parsing-the-results
    # https://esgf.github.io/esgf-user-support/user_guide.html#narrow-a-cmip5-data-search-to-just-one-variable
    center = []
    prefix = suffix = None
    facets = constraints.pop('facets', list(constraints))
    ctx = conn.new_context(facets=facets, **constraints)
    print('Context:', ctx.facet_constraints)
    print('Hit count:', ctx.hit_count)
    if ctx.hit_count == 0:
        print('Search returned no results.')
        return
    for i, ds in enumerate(ctx.search(batch_size=200)):
        fc = ds.file_context()
        fc.facets = ctx.facets  # TODO: report bug and remove?
        message = f'Dataset {i} (%s): {ds.dataset_id}'
        if dataset_filter and not dataset_filter(ds.dataset_id):
            print(message % 'skipped!!!')
            continue
        if flagship_filter and not flagship_filter(ds.dataset_id):
            print(message % 'nonflag!!!')
            continue
        try:
            script = fc.get_download_script(**constraints)
        except Exception:  # download failed
            print(message % 'failed!!!')
            continue
        if script.count('\n') <= 1:  # just an error message
            print(message % 'failed!!!')
            continue
        lines = _parse_script(script, complement=False)
        print(message % f'{len(lines)} files')
        if not prefix and not suffix:
            prefix, suffix = _parse_script(script, complement=True)
        center.extend(lines)

    # Create the script and return its path
    path = Path(path).expanduser() / 'unfiltered'
    dest = _write_script(
        path, prefix, center, suffix, openid=openid, printer=print, **constraints
    )
    return dest


def filter_script(
    path='~/data', maxyears=50, endyears=False, overwrite=False, project=None,
    facets_intersect=None, facets_folder=None, always_include=None, always_exclude=None,
    **constraints
):
    """
    Filter the wget scripts to the input number of years for intersecting
    facet constraints and group the new scripts into folders.

    Parameters
    ----------
    path : path-like, default: '~/data'
        The output path for the resulting data subfolder and wget file.
    maxyears : int, default: 50
        The number of years required for downloading.
    endyears : bool, default: False
        Whether to download from the start or end of the available times.
    overwrite : bool, optional
        Whether to overwrite the resulting datasets.
    facets_intersect : str or sequence, optional
        The facets that should be enforced to intersect across other facets.
    facets_folder : str or sequence, optional
        The facets that should be grouped into unique folders.
    always_include : dict-like, optional
        The constraints to always include in the output, ignoring the filters.
    always_exclude : dict-like, optional
        The constraints to always exclude from the output, ignoring the filters.
    **constraints
        Passed to `Printer` and `Database`.
    """
    # Read the file and group lines into dictionaries indexed by the facets we
    # want to intersect and whose keys indicate the remaining facets, then find the
    # intersection of these facets (e.g., 'ts_Amon_MODEL' for two experiment ids).
    # NOTE: Since _parse_constraints imposes a default project this will quietly
    # enforce that 'intersect' and 'folder' facets include a project identifier
    # (so the minimum folder indication will be e.g. 'cmip5' or 'cmip6').
    # NOTE: To overwrite previous files this script can simply be called with
    # overwrite=True. Then the file is removed so the wget script can be called
    # without -U and thus still skip duplicate files from the same node. Note
    # that if a file is not in the ESGF script cache then the wget command itself
    # will skip files that already exist unless ESGF_WGET_OPTS includes -O.
    project = (project or 'cmip6').lower()
    print = Printer('filter', project=project, **constraints)
    path = Path(path).expanduser() / 'unfiltered'
    files = sorted(path.glob(f'wget_{project.lower()}_*.sh'))
    print('Source file(s):')
    print('\n'.join(map(str, files)))
    print()
    prefix, suffix = _parse_script(files[0], complement=True)
    source = [line for file in files for line in _parse_script(file, complement=False)]
    facets_intersect = facets_intersect or FACETS_FOLDER
    database = Database(source, facets_intersect, project=project, **constraints)
    database.summarize(message='Initial groups', printer=print)
    groups = tuple(database.values())  # the group dictionaries
    keys = set(groups[0]).intersection(*map(set, groups))  # intersect dictionary keys
    database.filter(keys, always_include=always_include, always_exclude=always_exclude)
    database.summarize(message='Intersect groups', printer=print)

    # Collect the facets into a dictionary whose keys are the facets unique to
    # each folder and whose values are dictionaries with keys indicating the remaining
    # facets and values containing the associated script lines for subsequent filtering.
    # NOTE: Must use maxyears - 1 or else e.g. 50 years with 190001-194912
    # will not "satisfy" the range and result in the next file downloaded.
    dests = []
    facets_folder = facets_folder or FACETS_FOLDER
    source = [line for lines in database for line in lines]
    database = Database(source, facets_folder, project=project, **constraints)
    database.summarize(message='Folder groups', printer=print)
    for group, data in database.items():
        center = []  # wget script lines
        group = dict(zip(database.group, group))
        folder = path.parent / _item_join(group.values())  # e.g. cmip6-picontrol-amon
        kwargs = {facet: (opt,) for facet, opt in group.items()}
        for facet, opts in constraints.items():
            kwargs.setdefault(facet, opts)
        print('Writing download script:')
        print(', '.join(f'{key}: {value}' for key, value in group.items()))
        for key, lines in data.items():
            print('  ' + ', '.join(key) + ':', end=' ')
            years = (
                min((y for y, _ in map(_item_years, lines)), default=+10000),
                max((y for _, y in map(_item_years, lines)), default=-10000),
            )
            print('initial', '-'.join(map(str, years)), end=' ')
            if endyears:
                years = (int(max(years[0], years[1] - maxyears + 1)), years[1])
            else:
                years = (years[0], int(min(years[1], years[0] + maxyears - 1)))
            print('final', '-'.join(map(str, years)), end=' ')
            for line in lines:
                ys = _item_years(line)
                if ys[1] < years[0] or ys[0] > years[1]:
                    continue
                output = folder / _item_file(line)
                if output.is_file() and (overwrite or output.stat().st_size == 0):
                    print('removed {output.name!r}', end=' ')
                    output.unlink()
                center.append(line)
            print()
        dest = _write_script(folder, prefix, center, suffix, printer=print, **kwargs)
        dests.append(dest)
    return dests


def summarize_downloads(*paths, facets=None, remove=False, **constraints):
    """
    Compare the input netcdf files in the folder(s) to the files
    listed in the wget scripts in the same folder(s).

    Parameters
    ----------
    *paths : path-like, optional
        The folder(s) containing input files and wget scripts.
    facets : str, optional
        The facets to group by in the dataset.
    remove : bool, optional
        Whether to remove detected missing files. Use this option with caution!
    **constraints
        Passed to `Printer` and `Database`.
    """
    # Generate script and file databases
    # NOTE: This is generally used to remove unnecessarily downloaded files as users
    # refine their filtering or downloading steps. Slow because we check membership
    # in giant lists instead of intersecting smaller lists for each folder... but
    # this approach is more flexible and this only needs to be run once so no worries.
    facets = facets or FACETS_SUMMARY
    print = Printer('summary', 'downloads', **constraints)
    print('Generating databases.')
    files_downloaded, files_duplicate, files_corrupt = _glob_files(
        *paths, project=constraints.get('project', None),
    )
    names_scripts = [
        _item_file(line)
        for path in sorted(set(file.parent for file in files_downloaded))
        for file in path.glob('wget*.sh')
        for line in _parse_script(file, complement=False)
    ]
    database_downloads = Database(files_downloaded, facets, **constraints)
    database_scripts = Database(names_scripts, facets, **constraints)

    # Partition into separate databases
    print('All downloads.')
    database_downloads.summarize(missing=True, rawdata=False, printer=print)
    print('Unknown downloads.')
    database_unknown = copy.deepcopy(database_downloads)
    for group, data in database_unknown.items():
        for key, files in data.items():
            names = database_scripts.get(group, {}).get(key, ())
            files[:] = [file for file in files if file.name not in names]
    database_unknown.summarize(missing=False, rawdata=True, printer=print)
    print('Unfinished downloads.')
    database_unfinished = copy.deepcopy(database_scripts)
    for group, data in database_unfinished.items():
        for key, names in data.items():
            files = database_downloads.get(group, {}).get(key, ())
            files = [file.name for file in files]
            names[:] = [name for name in names if name not in files]
    database_unfinished.summarize(missing=False, rawdata=True, printer=print)

    # Remove unknown files and return file lists
    files_remove = (*database_unknown, *files_duplicate, *files_corrupt)
    if remove and files_remove:
        files_string = ', '.join(file.name for file in files_remove)
        response = input(f'Remove {len(file.name for file in files_remove)} files: {files_string} (y/[n])?')  # noqa: E501
        if response.lower().strip()[:1] == 'y':
            for file in files_remove:
                print(f'Removing file: {file}')
                file.unlink()
    files_downloads = sorted(file for files in database_downloads for file in files)
    return files_downloads, files_duplicate, files_corrupt
