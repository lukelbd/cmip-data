#!/usr/bin/env python3
"""
Download and filter datasets using the ESGF python API.

Important
---------
The scripts generated by these functions have two approaches for validating users:
either on-the-fly with the '-H' flag (requiring users to the password in a prompt)
or using some temporary certificate stored in '~/.esg/credentials.pem' (automatically
generated whenever pyesgf.LogonManager is instantiated). It seems only the latter
approach invokes 'wget' while former uses a less informative downloader. To use the
latter might have to delete the certificate file and instantiate a new logon manager.
"""
import builtins
import copy
import re
from pathlib import Path

from pyesgf.logon import LogonManager
from pyesgf.search import SearchConnection

from .facets import (
    _glob_files,
    _join_opts,
    _item_file,
    _item_years,
    _line_parts,
    _parse_constraints,
    _sort_facets,
    FacetPrinter,
    FacetDatabase,
    FACETS_FOLDER,
    FACETS_SUMMARY,
    FLAGSHIP_ENSEMBLES,
    NODES_HOSTS,
)

__all__ = [
    'init_connection',
    'download_script',
    'filter_script',
    'summarize_downloads',
]


def _parse_script(arg, complement=False):
    """
    Return the download lines of the wget scripts or their complement. The latter is
    used when constructing a single script from many scripts.

    Parameters
    ----------
    arg : str or path-like
        The string content or `pathlib.Path` location.
    complement : bool, default: False
        Whether to return the filename lines or the preceding and succeeding lines.
    """
    if isinstance(arg, Path):
        lines = open(arg, 'r').readlines()
    else:
        lines = [_ + '\n' for _ in arg.split('\n')]
    eof = 'EOF--dataset.file.url.chksum_type.chksum'  # marker for download lines
    idxs = [i for i, line in enumerate(lines) if eof in line]
    if idxs and len(idxs) != 2:
        raise NotImplementedError
    if complement:
        if not idxs:
            raise NotImplementedError
        return lines[:idxs[0] + 1], lines[idxs[1]:]  # return tuple of pairs
    else:
        if idxs:
            lines = lines[idxs[0] + 1:idxs[1]]
        return lines


def _write_script(
    path, prefix, center, suffix, openid=None, printer=None, **constraints
):
    """
    Save the wget script to the specified path. Lines are sorted first by model,
    second by variable, and third by node priority.

    Parameters
    ----------
    path : path-like
        The script location.
    prefix, center, suffix : list
        The script line parts. The center lines are sorted.
    openid : str, optional
        The openid to be hardcoded in the file.
    printer : callable, optional
        The print function.
    **constraints
        The constraints.
    """
    # NOTE: This also improves algorithm so that duplicate cache file entries and
    # in-place modifications to downloaded files do not trigger re-downloads.
    openid = openid or ''
    print = printer or builtins.print
    path = Path(path).expanduser()
    path.mkdir(exist_ok=True)
    _, constraints = _parse_constraints(**constraints)
    name = _join_opts(constraints.values())
    path = path / ('wget_' + name + '.sh')
    prefix = re.sub(r'openId=\n', f'openId={openid!r}\n', ''.join(prefix))
    if openid and not re.search(re.escape(f'{openid!r}'), prefix):
        print('Warning: Failed to substitute in user openid.')
    code = '"$(get_mod_time_ $file)" == $(echo "$cached" | cut -d \' \' -f2)'
    replace = r'-n "$(awk "$(get_mod_time_ $file) >= \$2 { print \$2 }" <<< "$cached")"'
    suffix = re.sub(re.escape(code), replace, ''.join(suffix))
    if not re.search(re.escape(replace), suffix):  # could already be true for filter
        print('Warning: Failed to repair modification time comparison line.')
    code = '"$chksum" == "$(echo "$cached" | cut -d \' \' -f3)"'
    replace = r'-n "$(awk "\"$chksum\" == \$3 { print \$3 }" <<< "$cached")"'
    suffix = re.sub(re.escape(code), replace, suffix)
    if not re.search(re.escape(replace), suffix):
        print('Warning: Failed to repair file checksum comparison line.')
    center = _sort_facets(center, tuple(_line_parts))  # sort with default order
    with open(path, 'w') as f:
        f.write(prefix + ''.join(center) + suffix)
    path.chmod(0o755)
    ntotal, nunique = len(center), len(set(map(_item_file, center)))
    print(f'Output script ({ntotal} total files, {nunique} unique files): {path}\n')
    return path


def init_connection(node='llnl', username=None, password=None):
    """
    Initialize a distributed search connection over the specified node
    with the specified user information.

    Parameters
    ----------
    node : str, default: 'llnl'
        The ESGF node to use for searching.
    username : str, optional
        The username for logging on.
    password : str, optional
        The password for logging on.
    """
    # Log on and initalize the connection using requested node
    node = node.lower()
    urls = NODES_HOSTS
    if node in urls:
        host = urls[node]
    elif node in urls.values():
        host = node
    else:
        raise ValueError(f'Invalid node {node!r}.')
    lm = LogonManager()
    if not lm.is_logged_on():  # surface orography
        lm.logon(username=username, password=password, hostname=host)
    url = f'https://{host}/esg-search'  # suffix is critical
    return SearchConnection(url, distrib=True)


def download_script(
    path='~/data', node='llnl', username=None, password=None, openid=None,
    dataset_filter=None, flagship_filter=False, **constraints
):
    """
    Download a wget script using `pyesgf`. The resulting script can be filtered to
    particular years or intersecting constraints using `filter_script`.

    Parameters
    ----------
    path : path-like, default: '~/data'
        The output path for the resulting wget file.
    node, username, password : str, defaultflagship_filter 'llnl'
        Passed to `connect_node`.
    openid : str, optional
        The openid to hardcode into the resulting wget script.
    dataset_filter : callable, optional
        Function that returns whether to retain a dataset id.
    flagship_filter : bool, optional
        Whether to select only flagship CMIP5 and CMIP6 ensembles.
    **constraints
        Constraints passed to `~pyesgf.search.SearchContext`.
    """
    # Translate constraints and possibly apply flagship filter
    # NOTE: The datasets returned by .search() will often be 'empty' but this usually
    # means it is not available from a particular node and available at another node.
    # NOTE: Thousands of these scripts can take up significant space... so instead we
    # consolidate results into a single script. Also increase the batch size from the
    # default of only 50 results to reduce the number of http requests.
    project, constraints = _parse_constraints(
        reverse=False, restrict=False, **constraints
    )
    raise Exception
    print = FacetPrinter('download', **constraints)
    conn = init_connection(node, username, password)
    facets = constraints.pop('facets', list(constraints))
    if flagship_filter:
        ensembles = [
            ensemble for key, ensemble in FLAGSHIP_ENSEMBLES.items() if key[0] == project  # noqa: E501
        ]
        if not ensembles:
            raise ValueError(f'Invalid {project=} for {flagship_filter=}. Must be CMIP5 or CMIP6.')  # noqa: E501
        _, constraints = _parse_constraints(
            reverse=False, restrict=False, ensemble=ensembles, **constraints
        )
        def flagship_filter(value):  # noqa: E301
            identifiers = [s.lower() for s in value.split('.')]
            for keys, ensemble in FLAGSHIP_ENSEMBLES.items():
                if all(key and key.lower() in identifiers for key in keys):
                    return True
            else:
                return FLAGSHIP_ENSEMBLES[project, None, None] in identifiers

    # Search and parse wget scripts
    # NOTE: This uses the 'dataset' search context to find individual simulations (i.e.
    # 'datasets' in ESGF parlance) then creates a 'file' search context within the
    # individual dataset and generates files form each list.
    # NOTE: Since cmip5 'datasets' often contain multiple variables, calling search()
    # on the DatasetSearchContext returned by new_context() then get_download_script()
    # on the resulting DatasetResult could include unwanted files. Therefore use
    # FileContext.constrain() to re-apply constraints to files within each dataset
    # (can also pass constraints to search() or get_download_script(), which both
    # call constrain() internally). This is in line with other documented approaches for
    # both the GUI and python API. See the below pages (the gitlab notebook also filters
    # out duplicate files but mentions the advantage of preserving all duplicates in
    # case one download node fails, which is what we use in our wget script workflow):
    # https://claut.gitlab.io/man_ccia/lab2.html#searching-and-parsing-the-results
    # https://esgf.github.io/esgf-user-support/user_guide.html#narrow-a-cmip5-data-search-to-just-one-variable
    center = []
    prefix = suffix = None
    ctx = conn.new_context(facets=facets, **constraints)
    print('Context:', ctx.facet_constraints)
    print('Hit count:', ctx.hit_count)
    if ctx.hit_count == 0:
        print('Search returned no results.')
        return
    for i, ds in enumerate(ctx.search(batch_size=200)):
        fc = ds.file_context()
        fc.facets = ctx.facets  # TODO: report bug and remove?
        message = f'Dataset {i} (%s): {ds.dataset_id}'
        if dataset_filter and not dataset_filter(ds.dataset_id):
            print(message % 'skipped!!!')
            continue
        if flagship_filter and not flagship_filter(ds.dataset_id):
            print(message % 'nonflag!!!')
            continue
        try:
            script = fc.get_download_script(**constraints)
        except Exception:  # download failed
            print(message % 'failed!!!')
            continue
        if script.count('\n') <= 1:  # just an error message
            print(message % 'failed!!!')
            continue
        lines = _parse_script(script, complement=False)
        print(message % f'{len(lines)} files')
        if not prefix and not suffix:
            prefix, suffix = _parse_script(script, complement=True)
        center.extend(lines)

    # Create the script and return its path
    path = Path(path).expanduser() / 'unfiltered'
    dest = _write_script(
        path, prefix, center, suffix, openid=openid, printer=print, **constraints
    )
    return dest


def filter_script(
    path='~/data', maxyears=50, endyears=False, overwrite=False,
    facets_intersect=None, facets_folder=None, flagship_translate=False,
    always_include=None, always_exclude=None, **constraints
):
    """
    Filter the wget scripts to the input number of years for intersecting
    facet constraints and group the new scripts into folders.

    Parameters
    ----------
    path : path-like, default: '~/data'
        The output path for the resulting data subfolder and wget file.
    maxyears : int, default: 50
        The number of years required for downloading.
    endyears : bool, default: False
        Whether to download from the start or end of the available times.
    overwrite : bool, optional
        Whether to overwrite the resulting datasets.
    facets_intersect : str or sequence, optional
        The facets that should be enforced to intersect across other facets.
    facets_folder : str or sequence, optional
        The facets that should be grouped into unique folders.
    flagship_translate : bool, optional
        Whether to group ensembles according to flagship or nonflagship identity.
    always_include : dict-like, optional
        The constraints to always include in the output, ignoring the filters.
    always_exclude : dict-like, optional
        The constraints to always exclude from the output, ignoring the filters.
    **constraints
        The constraints.
    """
    # Read the file and group lines into dictionaries indexed by the facets we
    # want to intersect and whose keys indicate the remaining facets, then find the
    # intersection of these facets (e.g., 'ts_Amon_MODEL' for two experiment ids).
    # NOTE: Since _parse_constraints imposes a default project this will quietly
    # enforce that 'intersect' and 'folder' facets include a project identifier (so
    # the minimum folder indication will be e.g. 'cmip5' or 'cmip6').
    # NOTE: To overwrite previous files this script can simply be called with
    # overwrite=True. Then the file is removed so the wget script can be called
    # without -U and thus still skip duplicate files from the same node. Note
    # that if a file is not in the ESGF script cache then the wget command itself
    # will skip files that already exist unless ESGF_WGET_OPTS includes -O.
    project, constraints = _parse_constraints(**constraints)
    print = FacetPrinter('filter', **constraints)
    path = Path(path).expanduser() / 'unfiltered'
    files = sorted(path.glob(f'wget_{project.lower()}_*.sh'))
    print('Source file(s):')
    print('\n'.join(map(str, files)))
    print()
    prefix, suffix = _parse_script(files[0], complement=True)
    source = [line for file in files for line in _parse_script(file, complement=False)]
    facets_intersect = facets_intersect or FACETS_FOLDER
    database = FacetDatabase(
        source, facets_intersect, flagship_translate=flagship_translate, **constraints,
    )
    database.summarize(message='Initial groups', printer=print)
    groups = tuple(database.values())  # the group dictionaries
    keys = set(groups[0]).intersection(*map(set, groups))  # intersect dictionary keys
    database.filter(keys, always_include=always_include, always_exclude=always_exclude)
    database.summarize(message='Intersect groups', printer=print)

    # Collect the facets into a dictionary whose keys are the facets unique to
    # each folder and whose values are dictionaries with keys indicating the remaining
    # facets and values containing the associated script lines for subsequent filtering.
    # NOTE: Must use maxyears - 1 or else e.g. 50 years with 190001-194912
    # will not "satisfy" the range and result in the next file downloaded.
    dests = []
    facets_folder = facets_folder or FACETS_FOLDER
    database.reset(facets_folder, project=project)
    database.summarize(message='Folder groups', printer=print)
    for group, data in database.items():
        center = []  # wget script lines
        folder = path.parent / _join_opts((group,))
        kwargs = {facet: (opt,) for facet, opt in zip(database.group, group)}
        for facet, opts in constraints.items():
            kwargs.setdefault(facet, opts)
        print('Writing script:', ', '.join(group))
        for key, lines in data.items():
            print('  ' + ', '.join(key) + ':', end=' ')
            years = (
                min((y for y, _ in map(_item_years, lines)), default=+10000),
                max((y for _, y in map(_item_years, lines)), default=-10000),
            )
            print('initial', '-'.join(map(str, years)), end=' ')
            if endyears:
                years = (int(max(years[0], years[1] - maxyears + 1)), years[1])
            else:
                years = (years[0], int(min(years[1], years[0] + maxyears - 1)))
            print('final', '-'.join(map(str, years)), end=' ')
            for line in lines:
                ys = _item_years(line)
                if ys[1] < years[0] or ys[0] > years[1]:
                    continue
                output = folder / _item_file(line)
                if output.is_file() and (overwrite or output.stat().st_size == 0):
                    print('removed {output.name!r}', end=' ')
                    output.unlink()
                center.append(line)
            print()
        dest = _write_script(folder, prefix, center, suffix, printer=print, **kwargs)
        dests.append(dest)
    return dests


def summarize_downloads(
    *paths, facets=None, remove=False, flagship_translate=False, **constraints
):
    """
    Compare the input netcdf files in the directory to the files
    listed in the wget scripts in the same directory.

    Parameters
    ----------
    *paths : path-like, optional
        The folder(s).
    facets : str, optional
        The facets to group by.
    remove : bool, optional
        Whether to remove detected missing files. Use this option with caution!
    flagship_translate : bool, optional
        Whether to group ensembles according to flagship or nonflagship identity.
    **constraints
        Passed to `_parse_constraints`.
    """
    # Generate script and file databases
    # NOTE: This is generally used to remove unnecessarily downloaded files as users
    # refine their filtering or downloading steps. Slow because we check membership
    # in giant lists instead of intersecting smaller lists for each folder... but
    # this approach is more flexible and this only needs to be run once so no worries.
    interval = 500
    facets = facets or FACETS_SUMMARY
    print = FacetPrinter('summary', 'downloads', **constraints)
    print('Generating databases.')
    files_downloaded, files_duplicate = _glob_files(
        *paths, project=constraints.get('project', None)
    )
    names_downloaded = [
        file.name for file in files_downloaded
    ]
    names_scripts = [
        _item_file(line)
        for path in sorted(set(file.parent for file in files_downloaded))
        for file in path.glob('wget*.sh')
        for line in _parse_script(file, complement=False)
    ]
    database_downloads = FacetDatabase(
        files_downloaded, facets, flagship_translate=flagship_translate, **constraints
    )
    database_scripts = FacetDatabase(
        names_scripts, facets, flagship_translate=flagship_translate, **constraints
    )

    # Partition into separate databases
    print('Partitioning finished downloads.')
    database_finished = copy.deepcopy(database_downloads)
    for i, files in enumerate(database_finished):
        i % interval or print(f'Files: {i} out of {len(database_finished)}')
        files[:] = [file for file in files if file.name in names_scripts]
    database_finished.summarize(missing=True, message='Finished downloads', printer=print)  # noqa: E501
    print('Partitioning unfinished downloads.')
    database_unfinished = copy.deepcopy(database_scripts)
    for i, names in enumerate(database_unfinished):
        i % interval or print(f'Files: {i} out of {len(database_unfinished)}')
        names[:] = [name for name in names if name not in names_downloaded]
    database_unfinished.summarize(missing=False, message='Unfinished downloads', printer=print)  # noqa: E501
    print('Partitioning unknown downloads.')
    database_unknown = copy.deepcopy(database_downloads)
    for i, files in enumerate(database_unknown):
        i % interval or print(f'Files: {i} out of {len(database_unknown)}')
        files[:] = [file for file in files if file.name not in names_scripts]
    database_unknown.summarize(missing=False, message='Unknown downloads', printer=print)  # noqa: E501

    # Remove unknown files and return file lists
    files_remove = (*database_unknown, *files_duplicate)
    if remove and files_remove:
        files_string = ', '.join(file.name for file in files_remove)
        response = input(f'Remove {len(file.name for file in files_remove)} files: {files_string} (y/[n])?')  # noqa: E501
        if response.lower().strip()[:1] == 'y':
            for file in files_remove:
                print(f'Removing file: {file}')
                file.unlink()
    files_finished = sorted(file for files in database_finished for file in files)
    files_unfinished = sorted(file for files in database_unfinished for file in files)
    files_unknown = sorted(file for files in database_unknown for file in files)
    return files_finished, files_unfinished, files_unknown, files_duplicate
