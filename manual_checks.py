#!/usr/bin/env python3
"""
Code for running various checks and post-processing on cmip data.
"""
import itertools
from pathlib import Path

import cftime
from cmip_data.download import _parse_script
from cmip_data.process import _parse_dump

# Constants
e3sm = False
dates = True
longs = False

# Summarize E3SM-1-0 files with different versions
# NOTE: This block is no longer valid, since we now automatically filter datasets
# to their latest available versions inside download_script(). Was originally used
# to help show which variables had multiple checksums available (turned out to be
# just the pressure level data hus, ta, ua, va, and zg), and the older versions of
# these files were manually found to have invalid data (e.g. minimum temperatures
# close to zero on levels with sub-surface grid cells, but the minimum varied, so did
# not seem to simply be an incorrectly encoded missing value like with GISS data).
if e3sm:
    print('Reading script files...')
    source = Path('~/scratch5').expanduser()
    scripts = sorted(source.glob('unfiltered/wget_cmip6_*.sh'))
    lines = tuple(line for script in scripts for line in _parse_script(script))
    files = {}
    debug = open('E3SM10.txt', 'w')
    debug.write('Summary of E3SM-1-0 model files with unique checksums.\n')
    debug.write('This was automatically generated by manual_checks.py\n\n')
    print('Categorizing script lines...')
    for experiment in ('piControl', 'abrupt-4xCO2'):
        for line in lines:
            var, tab, mod, exp, ens, _, date, *_ = (p.strip("'") for p in line.split('_'))  # noqa: E501
            if (tab, mod, exp) != ('Amon', 'E3SM-1-0', experiment):
                continue
            _, year = (p[:4] for p in date.split('.nc')[0].split('-'))
            if int(year) > 150:
                continue
            file = line.split()[0].strip("'")
            url = line.split()[1].strip("'").split('/')[2]
            sha = line.split()[3].strip("'")
            opts = files.setdefault(file, {})
            urls = opts.setdefault(sha, [])
            urls.append(url)
        for file, opts in files.items():
            debug.write(f'File: {file}\n')
            for i, urls in enumerate(opts.values()):
                debug.write(f'Checksum {i + 1}: ' + ' '.join(urls) + '\n')
            debug.write('\n')

# Summarize institutions and starting and ending dates of files
# TODO: Fix this... currently giving wrong answers.
# NOTE: This was written to try to figure out why Mark Zelinka omitted certain
# models and to ensure latest downloaded data represented the actual base time.
if dates:
    debug = open('DATES.txt', 'w')
    debug.write('Summary of date starts and institution ids.\n')
    debug.write('This was automatically generated by manual_checks.py\n')
    projects = {'cmip5': 'scratch2', 'cmip6': 'scratch5'}
    experiments = ('picontrol', 'abrupt4xco2')
    for project, base in projects.items():
        print(f'Reading {project.upper()} metadata...')
        base = Path(f'~/{base}').expanduser()
        starts = []
        models = set(
            file.name.split('_')[2]
            for experiment in experiments
            if (path := base / f'{project}-{experiment}-amon')
            for file in path.glob('ts_Amon_*')
        )
        for model, experiment in itertools.product(sorted(models), experiments):
            # Header
            path = base / f'{project}-{experiment}-amon'
            files = sorted(path.glob(f'ts_Amon_{model}_*'))
            if experiment == 'picontrol':
                units_child = calendar_child = start_parent = None
            if not files:
                continue
            print(model, len(files))
            if experiment == 'picontrol':
                debug.write('\n')
            debug.write(f'Project: {project.upper()} ')
            debug.write(f'Model: {model} ')
            debug.write(f'Experiment: {experiment}\n')
            attrs = _parse_dump(files[0])[2]
            info, time = attrs[''], attrs['time']
            # File information
            if experiment == 'picontrol':
                institute_id = info.get('institute_id', 'unknown')
                institute_id = info.get('institution_id', institute_id)
                debug.write(f'Institute: {institute_id}\n')
            start = files[0].name.split('_')[-1].split('-')[0][:6]
            start = f'{start[:4]}-{start[4:]}-01'
            debug.write(f'Start date: {start}\n')
            # Time information
            units_parent = info.get('parent_time_units', units_child)  # infer
            calendar_parent = info.get('parent_time_calendar', calendar_child)
            units_child = time.get('units', None)
            calendar_child = time.get('calendar', None)
            offset_parent = info.get('branch_time_in_parent')
            offset_child = info.get('branch_time', info.get('branch_time_in_child'))
            for title, offset, units, calendar in (
                ('', offset_child, units_child, calendar_child),
                ('Parent', offset_parent, units_parent, calendar_parent),
            ):
                if any(_ is None for _ in (units, calendar, offset)):
                    continue
                offset = int(float(str(offset).rstrip('SLDF')))
                prefix = f'{title} b' if title else 'B'
                date = cftime.num2date(0, units, calendar)
                date = date.strftime('%Y-%m-%d')
                debug.write(f'{prefix}ase date: {date}\n')
                date = cftime.num2date(offset, units, calendar)
                date = date.strftime('%Y-%m-%d')
                debug.write(f'{prefix}ranch date: {date} ')
                debug.write(f'(offset {offset} calendar {calendar})\n')
            # Post-processing
            if start_parent is None or offset_parent is None:
                start_parent = start
                continue
            offset_parent = int(float(str(offset_parent).rstrip('SLDF')))
            branch_parent = cftime.num2date(offset_parent, units, calendar)
            branch_parent = branch_parent.strftime('%Y-%m-%d')
            if start_parent != branch_parent:
                debug.write(f'WARNING: Start date {start_parent} differs from branch date {branch_parent}.\n')  # noqa: E501
                continue
